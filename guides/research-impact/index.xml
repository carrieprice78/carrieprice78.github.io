<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research Impact | Carrie Price, MLS</title>
    <link>https://carrieprice78.github.io/guides/research-impact/</link>
      <atom:link href="https://carrieprice78.github.io/guides/research-impact/index.xml" rel="self" type="application/rss+xml" />
    <description>Research Impact</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 12 Aug 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://carrieprice78.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Research Impact</title>
      <link>https://carrieprice78.github.io/guides/research-impact/</link>
    </image>
    
    <item>
      <title>Alternative Metrics</title>
      <link>https://carrieprice78.github.io/guides/research-impact/alternative-metrics/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://carrieprice78.github.io/guides/research-impact/alternative-metrics/</guid>
      <description>&lt;h2 id=&#34;what-are-alternative-metrics&#34;&gt;What are Alternative Metrics?&lt;/h2&gt;
&lt;p&gt;New technologies like social media and public repositories allow researchers to measure impact beyond the traditional h-index and impact factor. Alternative metrics make an estimate of impact through social media mentions, blog posts, media outlets, shares, saves, and downloads.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Altmetrics look at a variety of inputs, which vary depending on who is doing the measurement and how they have chosen to weigh each input relative to one another (and these complex weighting formulae are usually not disclosed). Typical inputs include activity on social networks and social bookmarking sites, mainstream media and blog coverage, and whether anyone has left any comments on the article.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;sub&gt;Reference: Crotty, D. (2017). Altmetrics. European Heart Journal, 38(35), 2647-2648 
&lt;a href=&#34;https://doi.org/10.1093/eurheartj/ehx447&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1093/eurheartj/ehx447&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;. . . that dog-eared (but uncited) article that used to live on a shelf now lives in Mendeley, CiteULike, or Zotero, where we can see and count it. That hallway conversation about a recent finding has moved to blogs and social networksâ€“now, we can listen in. The local genomics dataset has moved to an online repository; now, we can track it. This diverse group of activities forms a composite trace of impact far richer than any available before. We call the elements of this trace altmetrics&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;sub&gt;Reference: Priem, J., Taraborelli, D., Groth, P., &amp;amp; Neylon, C. (2010). 
&lt;a href=&#34;http://altmetrics.org/manifesto/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Altmetrics: A manifesto&lt;/a&gt;.&lt;/sub&gt;&lt;/p&gt;
&lt;h3 id=&#34;why-are-alternative-metrics-important&#34;&gt;Why are Alternative Metrics Important?&lt;/h3&gt;
&lt;p&gt;Alternative metrics give authors an immediate way to measure impact. Alternative metrics can provide supplemental measures of assessing impact, dissemination, and reach.&lt;/p&gt;
&lt;p&gt;See our selected tools for alternative metrics. Try a few of the tools and decide for yourself whether alternative metrics add value to your impact.&lt;/p&gt;
&lt;h2 id=&#34;tools-for-alternative-metrics&#34;&gt;Tools for Alternative Metrics&lt;/h2&gt;
&lt;h3 id=&#34;altmetric-bookmarklethttpswwwaltmetriccomproductsfree-toolsbookmarklet&#34;&gt;
&lt;a href=&#34;https://www.altmetric.com/products/free-tools/bookmarklet/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Altmetric Bookmarklet&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This free plugin for your browser allows you to see the social media shares and online mentions of an article with a single click.&lt;/p&gt;
&lt;h3 id=&#34;figsharehttpsfigsharecom&#34;&gt;
&lt;a href=&#34;https://figshare.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Figshare&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Figshare is a repository where users can make their research outputs available in a citable, shareable, and discoverable manner.&lt;/p&gt;
&lt;h3 id=&#34;impactstoryhttpsprofilesimpactstoryorg&#34;&gt;
&lt;a href=&#34;https://profiles.impactstory.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Impactstory&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;See mentions on Twitter, blogs, and news. Join with Twitter and make a profile to track the online impact of your research.&lt;/p&gt;
&lt;h3 id=&#34;inciteshttpsincitesclarivatecom&#34;&gt;
&lt;a href=&#34;https://incites.clarivate.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InCites&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A customized, web-based research evaluation tool that allows you to analyze institutional productivity and benchmark your output against peers worldwide. With customized citation data, global metrics, and multidimensional profiles on the leading research institutions, InCites gives comprehensive insight into your institution&amp;rsquo;s performance. And with robust visualization and reporting tools, you can create and share reports quickly and easily. Note: Users must create an account for initial login.&lt;/p&gt;
&lt;h3 id=&#34;incites-essential-science-indicatorshttpsesiclarivatecomindicatorsactionaction&#34;&gt;
&lt;a href=&#34;https://esi.clarivate.com/IndicatorsAction.action&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InCites Essential Science Indicators&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;InCites Essential Science Indicators provides access to a unique and comprehensive compilation of essential science performance statistics and science trend data. The chief indicators of output, or productivity, are journal article publication counts. Data in Essential Science Indicators are limited to Thomson Scientific-indexed journal articles only.&lt;/p&gt;
&lt;h3 id=&#34;mendeleyhttpswwwmendeleycominteraction_requiredtrue&#34;&gt;
&lt;a href=&#34;https://www.mendeley.com/?interaction_required=true&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mendeley&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Mendeley, a citation management program, has incorporated social streams into its platform. Users can keep track of saves by other Mendeley users.&lt;/p&gt;
&lt;h3 id=&#34;metrics-toolkithttpswwwmetrics-toolkitorg&#34;&gt;
&lt;a href=&#34;https://www.metrics-toolkit.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Metrics Toolkit&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Browse different types of metrics with the Metrics Toolkit and understand what they mean, how they are calculated, and if it is the right metric for you.&lt;/p&gt;
&lt;h3 id=&#34;publish-or-perishhttpsharzingcomresourcespublish-or-perish&#34;&gt;
&lt;a href=&#34;https://harzing.com/resources/publish-or-perish&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Publish or Perish&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Publish or Perish is a software program that retrieves and analyzes citations from a variety of sources, including Google Scholar.&lt;/p&gt;
&lt;h3 id=&#34;researchgatehttpswwwresearchgatenet&#34;&gt;
&lt;a href=&#34;https://www.researchgate.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ResearchGate&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;See reads, mentions, and citations of select publications using ResearchGate&amp;rsquo;s professional network.&lt;/p&gt;
&lt;h3 id=&#34;sciwheelhttpssciwheelcom&#34;&gt;
&lt;a href=&#34;https://sciwheel.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sciwheel&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Sciwheel is a citation management program that allows for collaboration and discussion.&lt;/p&gt;
&lt;h3 id=&#34;scopus&#34;&gt;Scopus&lt;/h3&gt;
&lt;p&gt;Scopus, a bibliographic literature database, also offers citation and field analysis and citation mapping. Perform a search, then click on the Analyze Search Results link on the results page.&lt;/p&gt;
&lt;h3 id=&#34;symplur-disease-hashtagshttpswwwsymplurcomhealthcare-hashtagsdiseases&#34;&gt;
&lt;a href=&#34;https://www.symplur.com/healthcare-hashtags/diseases/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Symplur Disease Hashtags&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Symplur aggregates trending Twitter hashtags related to diseases and conditions seeking to connect to others through Twitter.&lt;/p&gt;
&lt;h3 id=&#34;symplur-healthcare-hashtagshttpswwwsymplurcomhealthcare-hashtagsregular&#34;&gt;
&lt;a href=&#34;https://www.symplur.com/healthcare-hashtags/regular/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Symplur Healthcare Hashtags&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Symplur connects researchers and patients through Twitter by developing and organizing hashtags related to healthcare and medicine.&lt;/p&gt;
&lt;h3 id=&#34;web-of-sciencehttpsclarivatecomwebofsciencegroupsolutionsweb-of-science&#34;&gt;
&lt;a href=&#34;https://clarivate.com/webofsciencegroup/solutions/web-of-science/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Web of Science&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Web of Science, from Clarivate Analytics, offers citation and field analysis and citation mapping on topics. Perform a search, then click on Analyze Results on the results page.&lt;/p&gt;
&lt;h3 id=&#34;worldcat-first-searchhttpsfirstsearchoclcorg&#34;&gt;
&lt;a href=&#34;https://firstsearch.oclc.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WorldCat First Search&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;WorldCat allowsyou to see the number of libraries worldwide that hold a book in their collections.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Author Metrics</title>
      <link>https://carrieprice78.github.io/guides/research-impact/author-metrics/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://carrieprice78.github.io/guides/research-impact/author-metrics/</guid>
      <description>&lt;h2 id=&#34;author-metrics&#34;&gt;Author Metrics&lt;/h2&gt;
&lt;h3 id=&#34;the-h-index&#34;&gt;The h-Index&lt;/h3&gt;
&lt;p&gt;The h-index is a common metric that attempts to measure both the productivity and impact of the published work of a scientist or scholar. It was intended to be an unbiased way of comparing people within a discipline, especially in the sciences.  The h-index is a reflection of both the number of publication and the number of citations per publication. For example, an h-index of 25 means the researcher has published  at least 25 papers, each of which has been cited at least 25 or more times.&lt;/p&gt;
&lt;p&gt;The h-index can be found in Web of Science, Scopus, or Google. Your h-index may vary depending on which of these resources you consult because it is calculated based on the citations indexed by each.&lt;/p&gt;
&lt;p&gt;Caveats to using the h-index:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The h-index cannot be used to compare researchers in different disciplines due to differences in citations practices.&lt;/li&gt;
&lt;li&gt;The h-index can put early career researchers at a disadvantage since citations tend to increase over time&lt;/li&gt;
&lt;li&gt;should not be used as the only indicator to evaluate a researcher.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;finding-your-h-index-with-scopus&#34;&gt;Finding your h-Index with Scopus&lt;/h4&gt;
&lt;p&gt;Scopus has built-in features that make it easy to quickly find your h-index and create basic visualizations about a set of citations. You can use these features to discover the journals where you most frequently publish, view a network of your co-authors, and more.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To find your h-index in Scopus and view all of your citations, use the &amp;ldquo;Author&amp;rdquo; search feature to locate your author profile.&lt;/li&gt;
&lt;li&gt;Click on your name on the results page to view your &amp;ldquo;Author Details&amp;rdquo; page in Scopus.  Your h-index will be displayed to the right of your name.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;finding-your-h-index-with-web-of-science&#34;&gt;Finding your h-Index with Web of Science&lt;/h4&gt;
&lt;p&gt;Web of Science has built-in features that make it easy to quickly find your h-index and create basic visualizations about a set of citations. You can use these features to discover the journals where you most frequently publish, view a network of your co-authors, and more.&lt;/p&gt;
&lt;p&gt;See more from Web of Science and their 
&lt;a href=&#34;https://clarivate.libguides.com/woscc/analyze&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;author or search analytics functions on their guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Citation Report feature in Web of Science automatically calculates the h-index, average number of citations per publication, and the sum of all citations (with or without self-citations) for a group of citations. To find your own h-index, use the &amp;ldquo;Author Search&amp;rdquo; feature to locate all of your citations to use as the citation pool to be analyzed. Data can be exported to a Microsoft Excel file for further analysis.&lt;/p&gt;
&lt;h2 id=&#34;why-establish-a-researcher-profile&#34;&gt;Why Establish a Researcher Profile?&lt;/h2&gt;
&lt;p&gt;Creating and maintaining a unique public profile for the dissemination and promotion of your research is key to establishing your identity as a researcher. A researcher profile serves as a persistent identifier linking together your scholarly output throughout your career. A research profile can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Link all variations and changes to your name to one profile&lt;/li&gt;
&lt;li&gt;Disambiguate yourself from other researchers with the same or similar name&lt;/li&gt;
&lt;li&gt;Associate all of your publications throughout your career across multiple departments and institutional affiliations&lt;/li&gt;
&lt;li&gt;Enhance discoverability of your work by potential collaborators&lt;/li&gt;
&lt;li&gt;Make creating publication lists of your work fast and easy&lt;/li&gt;
&lt;li&gt;Allow you to take advantage of enhanced database features, such as tracking your h-index and and other research metrics&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;orcid&#34;&gt;ORCID&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://orcid.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ORCID&lt;/a&gt; is an acronym for &amp;ldquo;Open Researcher and Contributor ID.&amp;rdquo; ORCID is an independent, non-profit, community-based registry of unique research identifiers that allows researchers to create and maintain their own researcher profile. Researchers can include their education, work history, citations, and other research output in a single profile. Profile content can be imported by linking your ORCID profile to a Scopus Author ID or the Web of Science Publons, or by manually adding data from PubMed or other sources.&lt;/p&gt;
&lt;h3 id=&#34;sciencv&#34;&gt;SciENcv&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/sciencv/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SciENcv&lt;/a&gt;, the Science Experts Network Curriculum Vitae, is a researcher profile system for all individuals who apply for, receive, or are associated with research investments from U.S.federal agencies. SciENcv is available in My NCBI.&lt;/p&gt;
&lt;h3 id=&#34;google-scholar&#34;&gt;Google Scholar&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://scholar.google.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Scholar&lt;/a&gt; is a search engine that searches the web for scholarly and peer reviewed literature. Google Scholar allows authors to create and maintain a profile page. Sign in to your Google Account and you&amp;rsquo;ll be able to review your profile.&lt;/p&gt;
&lt;h3 id=&#34;scopus&#34;&gt;Scopus&lt;/h3&gt;
&lt;p&gt;Scopus, a blbliographic literature database from Elsevier, automatically creates an Author ID for authors as new citations are added to the database. An algorithm attempts to link citations with similar attributes to an Author ID. You can find your Author ID and make any corrections to the citations associated with your profile.&lt;/p&gt;
&lt;h3 id=&#34;publons-from-clarivate-analytics&#34;&gt;Publons from Clarivate Analytics&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://publons.com/about/home/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Publons (formerly ResearcherID)&lt;/a&gt; is the peer review activity tracker and researcher profile solution developed by Clarivate Analytics, the parent company of the Web of Science database. Upon registering, you are assigned a unique identifier you can use to manage your publication list, track citations to your work, and view your h-index, as well as a record of your work as a peer reviewer.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Article Metrics</title>
      <link>https://carrieprice78.github.io/guides/research-impact/article-metrics/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://carrieprice78.github.io/guides/research-impact/article-metrics/</guid>
      <description>&lt;h2 id=&#34;evolving-definitions-of-article-metrics&#34;&gt;Evolving Definitions of Article Metrics&lt;/h2&gt;
&lt;p&gt;Article metrics have traditionally been defined by the number of citations an article receives. In recent years, however, scholarly communities such as 
&lt;a href=&#34;https://sparcopen.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPARC&lt;/a&gt; and the 
&lt;a href=&#34;https://plos.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Public Library of Science (PloS)&lt;/a&gt; have advocated for additional metrics to be considered, alongside citation counts, when measuring the impact of an individual article or publication.&lt;/p&gt;
&lt;p&gt;According to SPARC, article metrics should ideally &amp;ldquo;aggregate a variety of data points that collectively quantify not only the impact of an article, but also the extent to which it has been socialized and its immediacy.&amp;rdquo; SPARC labels this definition of article metrics as Article-Level Metrics (ALMs). Researchers are encouraged to use traditional data points such as citation counts, along with other types of data points such as usage, mentions, and Altmetrics to quantify both the &amp;ldquo;scholarly visibility&amp;rdquo; and &amp;ldquo;social visibility&amp;rdquo; of an article. These different types of data points are visualized (right), and they are also discussed below. We encourage Hopkins researchers to follow this model when conveying article metrics.&lt;/p&gt;
&lt;p&gt;The Public Library of Science (PLoS) was the originator of Article-Level Metrics and they provide a robust set of resources and tools to facilitate the understanding and application of ALMs at 
&lt;a href=&#34;https://www.plos.org/article-level-metrics&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.plos.org/article-level-metrics&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;citation-tracking&#34;&gt;Citation Tracking&lt;/h2&gt;
&lt;p&gt;Citation tracking is used to assess the impact of articles or authors based on the number of times the article or authors have been cited by others. Citation tracking can be useful when assessing an author&amp;rsquo;s impact in a field or determining seminal papers on a topic. The graphic below shows the most highly-cited articles in the field of early mobility over a time span of decades. See example below demonstrating highly-cited articles in a field over time. Scopus, Web of Science, and Google Scholar all show a &amp;ldquo;cited by&amp;rdquo; number for an article.&lt;/p&gt;
&lt;h2 id=&#34;commonly-used-article-metrics&#34;&gt;Commonly-Used Article Metrics&lt;/h2&gt;
&lt;p&gt;Indvidual articles can be measured by the following indicators:&lt;/p&gt;
&lt;h3 id=&#34;citations&#34;&gt;Citations&lt;/h3&gt;
&lt;p&gt;Measures which articles have cited a particular article. Several different databases provide citation counts to discover this for individual publications.&lt;/p&gt;
&lt;h3 id=&#34;usage&#34;&gt;Usage&lt;/h3&gt;
&lt;p&gt;Downloads and page views from the publisher&amp;rsquo;s site, as well as from open access repositories when an article is published twice.&lt;/p&gt;
&lt;h3 id=&#34;captures&#34;&gt;Captures&lt;/h3&gt;
&lt;p&gt;Number of times bookmarked on CiteULike, shares on Mendeley, reads or downloads on ResearchGate.&lt;/p&gt;
&lt;h3 id=&#34;mentions&#34;&gt;Mentions&lt;/h3&gt;
&lt;p&gt;Number of appearances in blogs, news articles, wikipedia articles, and other similar online content.&amp;gt;&lt;/p&gt;
&lt;h3 id=&#34;social-media-mentions&#34;&gt;Social Media Mentions&lt;/h3&gt;
&lt;p&gt;Number of posts featuring the article on Twitter, Facebook, LinkedIn. See the Alternative Metrics section of this guide for more information.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Journal Metrics</title>
      <link>https://carrieprice78.github.io/guides/research-impact/journal-metrics/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://carrieprice78.github.io/guides/research-impact/journal-metrics/</guid>
      <description>&lt;h2 id=&#34;journal-metrics&#34;&gt;Journal Metrics&lt;/h2&gt;
&lt;p&gt;There are a variety of metrics that attempt to measure the individual journal performance. Journal metrics, such as the Journal Impact Factor by Clarivate Analytics, allow for ranking within a discipline. Authors may also use journal metrics to identify journals in which to publish. Two of the main sources for journal metrics are the Journal Citation Reports (Clarivate Analytics) and CiteScore (Scopus).&lt;/p&gt;
&lt;p&gt;Some caveats about journal metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Metrics may be easily skewed due factors such as the proportion of review articles a journal publishes (review articles tend to have a higher citation rate) and lack of data about potentially highly cited letters and editorials, and variables in citation practices across disciplines.&lt;/li&gt;
&lt;li&gt;The Journal Impact Factor was developed as a decision-aid to be used by the publishers of the Institute of Scientific Investigation (now part of Clarivate Analytics) to determine which journals to index and to help librarians with journal subscription purchase choices.  It was not meant to be an indicator of research impact.&lt;/li&gt;
&lt;li&gt;Metrics should not be used as an indicator of an individual article or author&amp;rsquo;s influence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;selection-of-indicators-included-in-the-journal-citation-report&#34;&gt;Selection of Indicators Included in the Journal Citation Report&lt;/h2&gt;
&lt;h3 id=&#34;journal-impact-factor&#34;&gt;Journal Impact Factor&lt;/h3&gt;
&lt;p&gt;The Journal Impact Factor is defined as all citations to the journal in the current JCR year to items published in the previous two years, divided by the total number of scholarly items (these comprise articles, reviews, and proceedings papers) published in the journal in the previous two years. Though not a strict mathematical average, the Journal Impact Factor provides a functional approximation of the mean citation rate per citable item.&lt;/p&gt;
&lt;h3 id=&#34;immediacy-index&#34;&gt;Immediacy Index&lt;/h3&gt;
&lt;p&gt;The Immediacy Index is the average number of times an article is cited in the year it is published. The Immediacy Index is calculated by dividing the number of times an article is cited in the year it is published.&lt;/p&gt;
&lt;p&gt;Because it is a per-article average, the Immediacy Index tends to discount the advantage of large journals over small ones. However, frequently issued journals may have an advantage because an article published early in the year has a better chance of being cited than one published later in the year. Many publications that publish infrequently or late in the year have low Immediacy Indexes.&lt;/p&gt;
&lt;h3 id=&#34;average-jif-percentile&#34;&gt;Average JIF Percentile&lt;/h3&gt;
&lt;p&gt;The Average Journal Impact Factor Percentile is based on the Journal Impact Factor Percentile (JIF Percentile). The JIF Percentile simply translates a journalâ€™s category rank into a percentile. For example, a journal that is ranked 19 out of 291 Biochemistry &amp;amp; Molecular Biology journals would receive a JIF Percentile score of 94. JIF Percentiles are scaled from 1 to 100 (higher values indicate higher Impact Factor in relation to other journals in the Category). The metric allows you to assess the journal&amp;rsquo;s standing in its own subject field.&lt;/p&gt;
&lt;p&gt;By factoring in a journal&amp;rsquo;s impact factor within its specific subject field, as well as controlling for the size of the field, the JIF Percentile is normalized and so allows comparison between journals in different subject areas. For example, assessing the impact of a medical journal against an engineering journal in a deeper way than simply comparing their respective Journal Impact Factors.&lt;/p&gt;
&lt;h3 id=&#34;eigenfactor&#34;&gt;Eigenfactor&lt;/h3&gt;
&lt;p&gt;The Eigenfactor Score attempts to provide a metric that measures the total influence of a scientific journal. Journals are rated according to the number of incoming citations, with citations from highly cited journals weighted to make a larger contribution to the Eigenfactor. The Eigenfactor score is intended to measure the importance of a journal to the scientific community, by considering the origin of the incoming citations, and is thought to reflect how frequently an average researcher will be redirected back to a journal by following the citations in the articles they find interesting. Typically, journals with a higher JIF have corresponding larger Eigenfactor scores. See the Eigenfactor Project for more information.&lt;/p&gt;
&lt;h2 id=&#34;journal-metrics-found-in-scopus&#34;&gt;Journal Metrics Found in Scopus&lt;/h2&gt;
&lt;h3 id=&#34;citescore&#34;&gt;CiteScore&lt;/h3&gt;
&lt;p&gt;The CiteScore is based on the average citations received per document. CiteScore is the number of citations received by a journal in one year to documents published in the three previous years, divided by the number of documents indexed in Scopus published in those same three years. CiteScoreâ€™s numerator and denominator both include all document types. This not only includes articles and reviews but also letters, notes, editorials, conference papers and other types indexed by Scopus. Therefore, the numerator and the denominator used in the CiteScore calculation are consistent. This approach gives a more complete picture of citation impact and makes manipulation of the calculation more difficult. Articles-in-press are indexed in Scopus for some publishers, but are not included in the CiteScore calculation.&lt;/p&gt;
&lt;h3 id=&#34;scimago-journal-rank-sjr&#34;&gt;Scimago Journal Rank (SJR)&lt;/h3&gt;
&lt;p&gt;This metric is a field-normalized metric that is an alternative to the Journal Impact Factor. It uses Scopus citation data to calculate the weighted prestige of a journal based on the number of citations received by a journal and the prestige of the journals where the citations originate.  With SJR, the subject field, quality and reputation of the journal has a direct effect on the value of a citation. Its method of calculation attempts to normalize for differences in citation behavior between subject fields.&lt;/p&gt;
&lt;h3 id=&#34;source-normalized-impact-per-papersnip&#34;&gt;Source Normalized Impact per Paper(SNIP)&lt;/h3&gt;
&lt;p&gt;SNIP is a field-normalized metric that measures a sourceâ€™s contextual citation impact by weighting citations based on the total number of citations in a subject field. It helps you make a direct comparison of sources in different subject fields.&lt;/p&gt;
&lt;p&gt;SNIP takes into account characteristics of the source&amp;rsquo;s subject field, which is the set of documents citing that source. SNIP especially considers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The frequency at which authors cite other papers in their reference lists&lt;/li&gt;
&lt;li&gt;The speed at which citation impact matures&lt;/li&gt;
&lt;li&gt;The extent to which the database used in the assessment covers the fieldâ€™s literature&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SNIP is the ratio of a source&amp;rsquo;s average citation count per paper and the citation potential of its subject field.&lt;/p&gt;
&lt;p&gt;The citation potential of a source&amp;rsquo;s subject field is the average number of references per document citing that source. It represents the likelihood of being cited for documents in a particular field. A source in a field with a high citation potential tends to have a high impact per paper.&lt;/p&gt;
&lt;p&gt;Citation potential is important because it accounts for the fact that typical citation counts vary widely between research disciplines. For example, they tend to be higher in life sciences than in mathematics or social sciences. If papers in one subject field contain an average of 40 cited references while those in another contain an average of 10, then the former field has a citation potential that is 4 times higher than that of the latter.&lt;/p&gt;
&lt;p&gt;Citation potential also varies between subject fields within a discipline. For instance, basic journals tend to show higher citation potentials than applied or clinical journals, and journals covering emerging topics tend to have higher citation potentials than periodicals in well-established areas&lt;/p&gt;
&lt;h3 id=&#34;citescore-percentile&#34;&gt;CiteScore Percentile&lt;/h3&gt;
&lt;p&gt;The CiteScore Perentile indicates the relative standing of a journal in its subject field. A CiteScore Percentile of 98% means the journal is in the top 2% of its subject field. You can use this number to compare journals in different subject fields.
CiteScore Rank and Rank Out Of&lt;/p&gt;
&lt;p&gt;The CiteScore Rank and Rank Out Of indicates the absolute standing of a serial in its field; for example, 14th out of 63 journals in the category.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Responsible Use</title>
      <link>https://carrieprice78.github.io/guides/research-impact/responsible-use/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://carrieprice78.github.io/guides/research-impact/responsible-use/</guid>
      <description>&lt;h2 id=&#34;the-leiden-manifesto&#34;&gt;The Leiden Manifesto&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;http://www.leidenmanifesto.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Leiden Manifesto for Research Metrics&lt;/a&gt; was published in 2015 by five experts urging responsible use in metrics, named after the conference where the idea came to fruition. They promote the following ten principles to guide research evaluation:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Quantitative evaluation should support qualitative, expert assessment.&lt;/li&gt;
&lt;li&gt;Measure performance against the research missions of the institution, group, or researcher.&lt;/li&gt;
&lt;li&gt;Protect excellence in locally relevant research.&lt;/li&gt;
&lt;li&gt;Keep data collection and analytical processes open, transparent, and simple.&lt;/li&gt;
&lt;li&gt;Allow those evaluated to verify data and analysis.&lt;/li&gt;
&lt;li&gt;Account for variation by field in publication and citation practices.&lt;/li&gt;
&lt;li&gt;Base assessment of individual researchers on a qualitative judgment of their portfolio.&lt;/li&gt;
&lt;li&gt;Avoid misplaced concreteness and false precision.&lt;/li&gt;
&lt;li&gt;Recognize the systemic effects of assessment and indicators.&lt;/li&gt;
&lt;li&gt;Scrutinize indicators regularly and update them.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Hicks, D., Wouters, P., Waltman, L., de Rijcke, S., &amp;amp; Rafols, I. (2015). Bibliometrics: The Leiden Manifesto for research metrics. Nature News 520(7548), 429â€“431. 
&lt;a href=&#34;https://doi.org/10.1038/520429a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1038/520429a&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;metric-tide-report&#34;&gt;Metric Tide Report&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://responsiblemetrics.org/the-metric-tide/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Metric Tide Report&lt;/a&gt;, published in 2015 commissioned by the Higher Education Funding Council for England (UK), is a report of the independent review of the role of metrics in research assessment and management. Traditional metrics have long been used as indicators for research and researcher impact. Their use can be problematic when taken out of context with uncritical acceptance. Responsible metrics should be considered and understood in the following dimensions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Robustness&lt;/strong&gt; - Is the metric using the best available and accurate data?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Humility&lt;/strong&gt; - Quantitative evaluation can complement, but not replace, expert assessment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transparency&lt;/strong&gt; - Is the collection of data and its analysis open to scrutiny?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Diversity&lt;/strong&gt; - Does the metric represent the landscape of research in any given field, and use appropriate indicators to support research and researchers?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reflexivity&lt;/strong&gt; - Is the use of bibliometric analysis dynamic and open to change?&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Metrics evoke a mixed reaction from the research community. A commitment to using data and evidence to inform decisions makes many of us sympathetic, even enthusiastic, about the prospect of granular, real-time analysis of our own activities. If we as a sector canâ€™t take full advantage of the possibilities of big data, then who can?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Wilsdon, J., et al. (2015). The Metric Tide: Report of the Independent Review of the Role of Metrics in Research Assessment and Management. 
&lt;a href=&#34;https://doi.org/10.13140/RG.2.1.4929.1363&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.13140/RG.2.1.4929.1363&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;sf-dora&#34;&gt;SF DORA&lt;/h2&gt;
&lt;p&gt;SF DORA&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sfdora.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The San Francisco Declaration on Research Assessment (SF DORA)&lt;/a&gt; recognizes the need to improve the ways in which the outputs of scholarly research are evaluated. The declaration was developed in 2012 during the Annual Meeting of the American Society for Cell Biology.&lt;/p&gt;
&lt;p&gt;The declaration is a worldwide initiative covering all scholarly disciplines and stakeholders.&lt;/p&gt;
&lt;p&gt;Signing the 2013 San Francisco Declaration on Research Assessment (DORA) is an important  way for individuals and organizations to publicly acknowledge their commitment to improve research by strengthening research assessment.&lt;/p&gt;
&lt;p&gt;The DORA roadmap for the next two years will focus on three strategic goals to enable signatories to take action:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Increase awareness of the need to develop credible alternatives to the inappropriate uses of metrics in research assessment.&lt;/li&gt;
&lt;li&gt;Research and promote tools and processes that facilitate best practice in research assessment.&lt;/li&gt;
&lt;li&gt;Extend the reach and impact of DORAâ€™s work across scholarly disciplines and in new areas of the world.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;From SF DORA at 
&lt;a href=&#34;https://sfdora.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;www.sfdora.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Read the declaration at SF DORA: 
&lt;a href=&#34;https://sfdora.org/read/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sfdora.org/read/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Sign the declaration at SF DORA: 
&lt;a href=&#34;https://sfdora.org/sign/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sfdora.org/sign/&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;further-reading-on-responsible-use-a-selected-bibliography&#34;&gt;Further Reading on Responsible Use: A Selected Bibliography&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Adair, S. M. (2006). Ethics in publishing: ghostwriting, conflicts of interest, and the impact factor. Pediatric Dentistry, 28(4), 309. Retrieved from 
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pubmed/16903437&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.ncbi.nlm.nih.gov/pubmed/16903437&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Alberts, B. (2013). Impact factor distortions. Science, 340(6134), 787. 
&lt;a href=&#34;https://doi.org/10.1126/science.1240319&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1126/science.1240319&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bohannon, J. (2016). Hate journal impact factors? New study gives you one more reason. Science. 
&lt;a href=&#34;https://doi.org/10.1126/science.aag0643&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1126/science.aag0643&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Charlier, P., Bridoux, V., Watier, L., MÃ©nÃ©trier, M., de la Grandmaison, G. L., HervÃ©, C. (2012). Ethics requirements and impact factor. Journal of Medical Ethics, 38(4), 253â€“255. 
&lt;a href=&#34;https://doi.org/10.1136/medethics-2011-100174&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1136/medethics-2011-100174&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Curry, S. (2018). Letâ€™s move beyond the rhetoric: it&amp;rsquo;s time to change how we judge research. Nature, 554(7691), 147. 
&lt;a href=&#34;https://doi.org/10.1038/d41586-018-01642-w&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1038/d41586-018-01642-w&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gallagher, A. (2011). The ethics of impact factors. Nursing Ethics, 18(1), 3â€“5. 
&lt;a href=&#34;https://doi.org/10.1177/0969733010388353&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1177/0969733010388353&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Heyeres, M., Tsey, K., Yang, Y., Yan, L., &amp;amp; Jiang, H. (2018). The characteristics and reporting quality of research impact case studies: A systematic review. Evaluation and Program Planning, 73, 10â€“23. 
&lt;a href=&#34;https://doi.org/10.1016/j.evalprogplan.2018.11.002&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1016/j.evalprogplan.2018.11.002&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hicks, D., Wouters, P., Waltman, L., de Rijcke, S., &amp;amp; Rafols, I. (2015). Bibliometrics: The Leiden Manifesto for research metrics. Nature, 520(7548), 429â€“431. 
&lt;a href=&#34;https://doi.org/10.1038/520429a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1038/520429a&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Huggett, S. (2013). Journal bibliometrics indicators and citation ethics: a discussion of current issues. Atherosclerosis, 230(2), 275â€“277. 
&lt;a href=&#34;https://doi.org/10.1016/j.atherosclerosis.2013.07.051&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1016/j.atherosclerosis.2013.07.051&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jones, J. F. X. (2013). The impact of impact factors and the ethics of publication. Irish Journal of Medical Science, 182(4), 541. 
&lt;a href=&#34;https://doi.org/10.1007/s11845-013-1014-y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1007/s11845-013-1014-y&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Simons, K. (2008). The misused impact factor. Science, 322(5899), 165. 
&lt;a href=&#34;https://doi.org/10.1126/science.1165316&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1126/science.1165316&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Teixeira da Silva, J. A. (2017). The ethics of peer and editorial requests for self-citation of their work and journal. Armed Forces Medical Journal, India, 73(2), 181â€“183. 
&lt;a href=&#34;https://doi.org/10.1016/j.mjafi.2016.11.008&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://doi.org/10.1016/j.mjafi.2016.11.008&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
